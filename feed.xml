<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rahmanidashti.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://rahmanidashti.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-10-02T14:15:52+00:00</updated><id>https://rahmanidashti.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Synthetic Test Collection for Evaluation</title><link href="https://rahmanidashti.github.io/blog/2024/paper-synthetic-test/" rel="alternate" type="text/html" title="Synthetic Test Collection for Evaluation" /><published>2024-07-04T22:40:16+00:00</published><updated>2024-07-04T22:40:16+00:00</updated><id>https://rahmanidashti.github.io/blog/2024/paper-synthetic-test</id><content type="html" xml:base="https://rahmanidashti.github.io/blog/2024/paper-synthetic-test/"><![CDATA[<h3 id="can-we-use-large-language-models-llms-to-build-a-reliable-synthetic-test-collection">Can we use Large Language Models (LLMs) to build a reliable Synthetic Test Collection?</h3>

<p>Many recent studies have explored generating synthetic data using Large Language Models (LLMs) in various domains such as Computer Vision (CV) and Information Retrieval (IR). While previous work in IR (see InPars [<a href="#references">1</a>]) exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data for our-of-domain and our-of-distribution generalisation, using LLMs for constructing Synthetic Test Collections is relatively unexplored.</p>

<h4 id="why-construction-synthetic-test-collection">Why construction Synthetic Test Collection?</h4>
<p>Test collections play a vital role in the evaluation of IR systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often <strong>costly</strong> and <strong>resource-intensive</strong>.</p>

<p>In our recent research study [<a href="#references">2</a>], we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating synthetic queries and synthetic judgments. To generate the synthetic test collection, we proposed the following generation pipeline:</p>

<div class="row mt-6">
    <div class="col-sm mt-6 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/synthetic-test-generation-piepline-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/synthetic-test-generation-piepline-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/synthetic-test-generation-piepline-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/24-sigir-synthetic-data/synthetic-test-generation-piepline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1. Synthetic Test Generation Piepline
</div>

<ol>
  <li><span class="font-weight-bold">Passage Selection:</span> We first randomly sampled 1000 passages from the MSMARCO v2 passage corpus.</li>
  <li><span class="font-weight-bold">Passage Filtering:</span> We then filtered for passages that could be good stand-alone search results using GPT-4.</li>
  <li><span class="font-weight-bold">Query Generation:</span> We generated queries using (i) a pre-trained T5-based query generation model from BeIR and (ii) a zero-shot query generation approach using GPT-4.</li>
  <li><span class="font-weight-bold">Query Selection:</span> We sampled the T5 query-passage pairs to match a target sample of positive qrels from the 2022 passage task; NIST assessors further removed queries that did not look reasonable and contained too few or too many relevant documents.</li>
  <li><span class="font-weight-bold">Relevance Judgment Generation:</span> We used GPT-4 to automatically label the documents (that were originally annotated using NIST annotators) for the synthetic queries to generate synthetic relevance judgements.</li>
</ol>

<p>We compare our fully synthetic test collation with real test collection on system evaluation ordering to see if it is possible to obtain evaluation results that are similar to results obtained using real test collections. Our experiments indicate that by using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation:</p>

<div class="row mt-2">
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-queries-synthetic-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-queries-synthetic-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-queries-synthetic-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-queries-synthetic.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-2 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-full-synthetic-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-full-synthetic-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-full-synthetic-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/24-sigir-synthetic-data/full-real-vs-full-synthetic.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2. Scatter plot of the effectiveness (i.e., NDCG@10) of TREC DL 2023 runs according to the (1) real test collection and (b) synthetic test collection with human judgments. A point represents a single run averaged over all queries.
</div>

<p>We evaluate the quality of the 31 systems from TREC DL 2023 using official judgments obtained from expert human assessors from NIST, and compare the ranking of these systems on real queries and synthetic queries. Figure 2(a) shows how the performance of systems using synthetically generated queries compares with system performance on real queries. It can be seen that synthetic queries and real queries show similar patterns in terms of evaluation results and system ranking, with a system ordering agreement of Kendall’s \(\tau\) = 0.8151. More interestingly, when we extend our synthetic test collection to a fully synthetic test collection by generating synthetic relevance judgment the agreement shows a higher value, see Figure 2(b) Kendall’s \(\tau\) value of 0.8568.</p>

<h5 id="what-could-be-the-major-issue-when-we-use-the-synthetic-test-collection-for-evaluation">What could be the major issue when we use the Synthetic Test Collection for evaluation?</h5>
<p>One potential issue with using fully synthetic test collection construction is the possible bias these collections may exhibit towards systems that are based on a similar approach (similar language model) to the one that was used in the synthetic test collection construction process.</p>

<p>In order to answer this question, we extend our experiments by categorising the systems based on the language models or the architecture they used in their pipeline. This results in four types of systems: systems based on <code class="language-plaintext highlighter-rouge">GPT</code>, <code class="language-plaintext highlighter-rouge">T5</code>, <code class="language-plaintext highlighter-rouge">GPT + T5</code> (i.e., a combination of GPT and T5), and <code class="language-plaintext highlighter-rouge">others</code> (i.e., traditional methods such as BM25, or any model that does not use either GPT or T5).</p>

<div class="row mt-6">
    <div class="col-sm mt-6 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/bias-analysis-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/bias-analysis-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/24-sigir-synthetic-data/bias-analysis-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/24-sigir-synthetic-data/bias-analysis.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3: Scatter plots of the effectiveness of TREC DL 2023 runs based on synthetic vs. real test collections to analyse the bias towards systems using the same language model as the one used in synthetic test collection construction.
</div>

<p>While previous studies (see G-Eval [<a href="#references">3</a>] paper for more detailed analysis) on LLM evaluation discussed the potential bias towards LLM-generated text when we use LLMs for evaluation, in our experiments we did not observe very clear evidence of systematic bias. Our experiments (see Figure 3) show that the synthetic test collection we have constructed that contains synthetic queries generated by LLMs (T5 and GPT-4) exhibits little to no bias towards LLM-based systems.</p>

<h4 id="references">References</h4>
<ol>
  <li>
    <p>Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. “Inpars: Unsupervised dataset generation for information retrieval.”In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2387-2392. 2022.</p>
  </li>
  <li>
    <p>Hossein A. Rahmani, Nick Craswell, Emine Yilmaz, Bhaskar Mitra, and Daniel Campos. “Synthetic Test Collections for Retrieval Evaluation.” arXiv preprint arXiv:2405.07767 (2024).</p>
  </li>
  <li>
    <p>Liu, Yang, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. “G-eval: Nlg evaluation using gpt-4 with better human alignment.” arXiv preprint arXiv:2303.16634 (2023).</p>
  </li>
</ol>]]></content><author><name></name></author><category term="Research" /><category term="Synthetic" /><category term="Data" /><category term="LLM" /><category term="Evaluation" /><summary type="html"><![CDATA[A short blog on Synthetic Test Collection]]></summary></entry><entry><title type="html">SIGIR 2023 Statistics</title><link href="https://rahmanidashti.github.io/blog/2023/sigir23-stats/" rel="alternate" type="text/html" title="SIGIR 2023 Statistics" /><published>2023-07-26T22:40:16+00:00</published><updated>2023-07-26T22:40:16+00:00</updated><id>https://rahmanidashti.github.io/blog/2023/sigir23-stats</id><content type="html" xml:base="https://rahmanidashti.github.io/blog/2023/sigir23-stats/"><![CDATA[<div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">This blog post is inspired from Hamed's earlier blog posts for SIGIR reports. 
</span></code></pre></div></div>

<p>Unfortunately, I did not have the chance to attend SIGIR 2023 this year in Taiwan! I almost was following the updates from SIGIR on Twitter, thanks to the ones who were sharing the amazing and inspiring moments at SIGIR 2023 and congratulation to all the winners!</p>

<p>However, I thought that would be great if we have a very short stat of SIGIR this year! This is the first time that I post the state of a conference like SIGIR! However, we know that during the past years, <a href="https://groups.cs.umass.edu/zamani/" target="_blank">Hamed Zamani</a> and <a href="https://cs.uwaterloo.ca/~jimmylin/" target="_blank">Jimmy Lin</a> reported the stats of SIGIR conferences. Here is the link to the <a href="https://gist.github.com/lintool/c2a95110499302609cee73c423e0d971" target="_blank">SIGIR 2019</a>, <a href="https://groups.cs.umass.edu/zamani/2020/07/02/sigir-2020-stats/" target="_blank">SIGIR 2020</a>, and <a href="https://groups.cs.umass.edu/zamani/2021/04/29/sigir-2021-stats/" target="_blank">SIGIR 2021</a> report.</p>

<blockquote>
  <p>The following information and summarisation have been automatically extracted from <a href="https://sigir.org/sigir2023/" target="_blank">the full list of accepted papers of all tracks</a> to ACM SIGIR 2023. Please let me know if you find any errors in the reported statistics.</p>
</blockquote>

<blockquote>
    Note that the <a href="https://sigir.org/sigir2023/" target="_blank">SIGIR website</a> only released the title and the name of the authors of the accepted papers. Therefore, there is no affiliation/institution-level analysis in this report.
</blockquote>

<p>First, we can take a look at a WordCloud I created from the titles of the accepted papers (all tracks, excluding Doctoral Consortium papers):</p>

<div class="row mt-6">
    <div class="col-sm mt-6 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/sigir-2023-title-wordcloud-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/sigir-2023-title-wordcloud-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/sigir-2023-title-wordcloud-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/sigir-2023-title-wordcloud.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The WordCloud of SIGIR 2023 Paper Titles
</div>

<p>The WordCloud of the titles of all papers show a high frequency of words such as <code class="language-plaintext highlighter-rouge">Recommendation</code>, <code class="language-plaintext highlighter-rouge">Learning</code>, <code class="language-plaintext highlighter-rouge">Search</code>, <code class="language-plaintext highlighter-rouge">Retrieval</code>, <code class="language-plaintext highlighter-rouge">User</code>, <code class="language-plaintext highlighter-rouge">text</code>, and <code class="language-plaintext highlighter-rouge">Graph</code> is not very surprising as we have seen a similar pattern from the <a href="https://groups.cs.umass.edu/zamani/2021/04/29/sigir-2021-stats/" target="_blank">SIGIR 2021 report</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/SIGIR_2023_Title_Full_WordCloud-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/SIGIR_2023_Title_Full_WordCloud-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/SIGIR_2023_Title_Full_WordCloud-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/SIGIR_2023_Title_Full_WordCloud.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/SIGIR_2023_Title_SIRIP_WordCloud-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/SIGIR_2023_Title_SIRIP_WordCloud-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/SIGIR_2023_Title_SIRIP_WordCloud-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/SIGIR_2023_Title_SIRIP_WordCloud.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/SIGIR_2023_Title_Doc_WordCloud-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/SIGIR_2023_Title_Doc_WordCloud-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/SIGIR_2023_Title_Doc_WordCloud-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blogs/SIGIR_2023_Title_Doc_WordCloud.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The WordCloud of (a) Full, (b) SIRIP, (c) Doctoral Consortium papers
</div>

<p>Then, we can see the WordCloud for full (165 papers), SIRIP-Industry (40 papers), and Doctoral Consortium (11 papers) papers. We can see a higher frequency of the words <code class="language-plaintext highlighter-rouge">Graph</code> and <code class="language-plaintext highlighter-rouge">Learning</code> in full papers while they are not very frequent in SIRIP papers. However, we can see the word <code class="language-plaintext highlighter-rouge">Online</code> that indicates the importance of online experiments and evaluation. More interestingly, doctoral consortium papers show the high frequency of the words <code class="language-plaintext highlighter-rouge">Large</code> and <code class="language-plaintext highlighter-rouge">Augmentation</code> that indicate a high prevalence of LLM-related PhD topics.</p>

<h5 id="top-10-authors--all-papers-full-short-perspective-reproducibility-demo-resource-sirip-industrial-tracks--based-on-normalized-publication-count">TOP 10 AUTHORS – All Papers (Full, Short, Perspective, Reproducibility, Demo, Resource, SIRIP Industrial tracks) – Based on Normalized Publication Count</h5>

<ul>
  <li><strong>#1</strong> Maarten De Rijke, Professor, University of Amsterdam (score: 2.48)</li>
  <li><strong>#2</strong> Hamed Zamani, Assistant Professor, UMass Amherst (score: 1.78)</li>
  <li><strong>#3</strong> Sean Macavaney, Assistant Professor, University of Glasgow (score: 1.52)</li>
  <li><strong>#4</strong> Jun Zhou, Ant Group, China (score: 1.42)</li>
  <li><strong>#5</strong> Jimmy Lin, Professor, University of Waterloo (score: 1.31)</li>
  <li><strong>#6</strong> Riku Togashi, CyberAgent Inc., Japan (score: 1.25)</li>
  <li><strong>#7</strong> Harrisen Scells, Research Fellow, Leipzig University (score: 1.2)</li>
  <li><strong>#7</strong> Aixin Sun, Associate Professor, Nanyang Technological University (score: 1.2)</li>
  <li><strong>#8</strong> Martin Potthast, Professor, Leipzig University (score: 1.18)</li>
  <li><strong>#9</strong> Michael Bendersky, Google Research, US (score: 1.14)</li>
</ul>

<p><em>Note that the normalised publication count socres were computed by normalizing publication counts by the number of authors per paper.</em></p>

<h5 id="top-10-authors--all-papers-full-short-perspective-reproducibility-demo-resource-sirip-industrial-tracks--based-on-publication-count">TOP 10 AUTHORS – All Papers (Full, Short, Perspective, Reproducibility, Demo, Resource, SIRIP Industrial tracks) – Based on Publication Count</h5>

<ul>
  <li><strong>#1</strong> Maarten De Rijke, Professor, University of Amsterdam (10 papers)</li>
  <li><strong>#1</strong> Jun Zhou, Ant Group, China (10 papers)</li>
  <li><strong>#2</strong> Fuzhen Zhuang, Professor, Beihang University (8 papers)</li>
  <li><strong>#3</strong> Xiangyu Zhao, Assistant Professor, City University of Hong Kong (7 papers)</li>
  <li><strong>#4</strong> Hamed Zamani, Assistant Professor, UMass Amherst (6 papers)</li>
  <li><strong>#4</strong> Liqiang Nie, Professor, Shandong University (6 papers)</li>
  <li><strong>#4</strong> Zhenhua Dong, Noah’s ark lab, Huawei Technologies Co. (6 papers)</li>
  <li><strong>#4</strong> Ruiming Tang, Noah’s Ark Lab, Huawei Technologies Co. (6 papers)</li>
  <li><strong>#4</strong> Jimmy Lin, Professor, University of Waterloo (6 papers)</li>
  <li><strong>#5</strong> Yiqun Liu, Professor, Tsinghua University (5 papers)</li>
  <li><strong>#5</strong> Tat-Seng Chua, Professor, National University of Singapore (5 papers)</li>
  <li><strong>#5</strong> Sean Macavaney, Assistant Professor, University of Glasgow (5 papers)</li>
  <li><strong>#5</strong> Martin Potthast, Professor, Leipzig University (5 papers)</li>
  <li><strong>#5</strong> Enhong Chen, Professor, University of Science and Technology of China (5 papers)</li>
</ul>

<h4 id="top-10-authors--only-full-papers--based-on-normalized-publication-count">TOP 10 AUTHORS – Only full papers – Based on Normalized Publication Count</h4>

<ul>
  <li><strong>#1</strong> Hamed Zamani, Assistant Professor, UMass Amherst (score: 1.78)</li>
  <li><strong>#2</strong> Maarten De Rijke, Professor, University of Amsterdam (score: 1.65)</li>
  <li><strong>#3</strong> Naoto Ohsaka, CyberAgent Inc., Japan (score: 1.0)</li>
  <li><strong>#3</strong> Riku Togashi, CyberAgent Inc., Japan (score: 1.0)</li>
  <li><strong>#3</strong> Andrés Hoyos-Idrobo, Rakuten Group Inc., France (score: 1.0)</li>
  <li><strong>#4</strong> Liqiang Nie, Professor, Shandong University (score: 0.88)</li>
  <li><strong>#4</strong> Tat-Seng Chua, Professor, National University of Singapore (score: 0.88)</li>
  <li><strong>#5</strong> Fuzhen Zhuang, Professor, Beihang University (score: 0.84)</li>
  <li><strong>#6</strong> Yi Cai, Professor, South China University of Technology (score: 0.75)</li>
  <li><strong>#7</strong> Hongzhi Yin, Associate Professor, University of Queensland (score: 0.71)</li>
</ul>

<p><em>Note that the normalised publication count socres were computed by normalizing publication counts by the number of authors per paper.</em></p>

<h4 id="top-10-authors--only-full-papers--based-on-publication-count">TOP 10 AUTHORS – Only full papers – Based on Publication Count</h4>

<ul>
  <li><strong>#1</strong> Maarten De Rijke, Professor, University of Amsterdam (7 papers)</li>
  <li><strong>#2</strong> Hamed Zamani, Assistant Professor, UMass Amherst (6 papers)</li>
  <li><strong>#2</strong> Fuzhen Zhuang, Professor, Beihang University (6 papers)</li>
  <li><strong>#3</strong> Liqiang Nie, Professor, Shandong University (5 papers)</li>
  <li><strong>#3</strong> Xiangyu Zhao, Assistant Professor, City University of Hong Kong (5 papers)</li>
  <li><strong>#3</strong> Tat-Seng Chua, Professor, National University of Singapore (5 papers)</li>
  <li><strong>#4</strong> Xiangnan He, Professor, University of Science and Technology of China (4 papers)</li>
  <li><strong>#4</strong> Zhenhua Dong, Noah’s ark lab, Huawei Technologies Co (4 papers)</li>
  <li><strong>#4</strong> Ruiming Tang, Noah’s Ark Lab, Huawei Technologies Co (4 papers)</li>
  <li><strong>#4</strong> Hongzhi Yin, Associate Professor, University of Queensland (4 papers)</li>
</ul>]]></content><author><name></name></author><category term="report" /><category term="SIGIR" /><category term="Statistics" /><summary type="html"><![CDATA[A short statistics on SIGIR 2023]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://rahmanidashti.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog" /><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://rahmanidashti.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://rahmanidashti.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>